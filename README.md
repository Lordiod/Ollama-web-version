<<<<<<< HEAD
# ðŸ¤– Ollama AI Assistant - Modern Web Interface

A beautiful, modern web interface for Ollama with real-time streaming responses, persistent chat history, and ChatGPT-style UI. Built with Next.js, TypeScript, and Tailwind CSS.

![Ollama AI Assistant](./public/preview.png)

## âœ¨ Features

### ðŸŽ¨ Modern Design
- **Glassmorphic UI** with beautiful gradients and animations
- **Responsive design** that works on desktop and mobile
- **Professional color schemes** with smooth transitions
- **Micro-interactions** and smooth animations

### ðŸ’¬ Advanced Chat Experience
- **Real-time streaming** responses with typing effect
- **ChatGPT-style interface** - centered input on welcome, bottom on chat
- **Interactive suggestion cards** for quick prompts
- **Smart auto-scroll** that respects user position
- **Character-by-character streaming** for natural conversation flow

### ðŸ“š Persistent Chat History
- **Session management** with automatic saving
- **Chat history sidebar** with organization
- **Cross-session memory** - AI remembers context across logins
- **Smart chat titles** generated from first user message
- **Delete confirmation** for chat safety

### ðŸ” User Authentication
- **Secure login/signup** with email validation
- **Session persistence** across browser sessions
- **User-specific chat isolation**
- **Animated auth forms** with modern design

### ðŸš€ Performance & Technical
- **TypeScript** for type safety and better DX
- **Next.js 14** with optimized builds
- **Supabase** for database and auth
- **Real-time streaming** with proper error handling
- **Fast loading** and optimized performance
=======
# Ollama Web Assistant

A modern, full-stack web application that provides an intuitive chat interface for Ollama AI models with user authentication and persistent chat history.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![TypeScript](https://img.shields.io/badge/TypeScript-Ready-blue.svg)](https://www.typescriptlang.org/)
[![Next.js](https://img.shields.io/badge/Next.js-14-black.svg)](https://nextjs.org/)
[![React](https://img.shields.io/badge/React-18-blue.svg)](https://reactjs.org/)

## Features

- **Local AI Integration**: Connect to Ollama models running locally for private, fast inference
- **Secure Authentication**: User registration and login system powered by Supabase
- **Chat Persistence**: Automatic saving and loading of conversation history
- **Responsive Design**: Modern UI built with Tailwind CSS that works on all devices
- **Real-time Chat**: Instant messaging with streaming responses
- **Type Safety**: Full TypeScript implementation for better developer experience
- **Session Management**: Advanced chat session handling with persistent storage
>>>>>>> 3412d79ed96ea76df9ca04d59038bf81ce54e724

## Tech Stack

### Frontend
- **Next.js 14** - React framework with App Router
- **React 18** - UI library with modern hooks
- **TypeScript** - Type safety and better DX
- **Tailwind CSS** - Utility-first CSS framework

### Backend
- **Next.js API Routes** - Serverless API endpoints
- **Supabase** - Database and authentication
- **Ollama** - Local AI model inference

### Development
- **ESLint** - Code linting and formatting
- **TypeScript** - Static type checking

## Project Structure

```
ollama-web-version/
â”œâ”€â”€ components/             # Reusable React components
â”‚   â”œâ”€â”€ Auth.tsx           # Authentication forms and logic
â”‚   â””â”€â”€ Chat.tsx           # Main chat interface
â”œâ”€â”€ lib/                   # Utility libraries and configurations
â”‚   â””â”€â”€ supabase.ts        # Supabase client configuration
â”œâ”€â”€ pages/                 # Next.js pages and API routes
â”‚   â”œâ”€â”€ api/               # Backend API endpoints
â”‚   â”‚   â”œâ”€â”€ auth/          # Authentication handlers
â”‚   â”‚   â”‚   â”œâ”€â”€ login.ts   # User login endpoint
â”‚   â”‚   â”‚   â””â”€â”€ signup.ts  # User registration endpoint
â”‚   â”‚   â””â”€â”€ chat/          # Chat-related API endpoints
â”‚   â”‚       â”œâ”€â”€ clear.ts   # Clear chat history
â”‚   â”‚       â”œâ”€â”€ delete.ts  # Delete specific chats
â”‚   â”‚       â”œâ”€â”€ generate-title.ts # Generate chat titles
â”‚   â”‚       â”œâ”€â”€ load.ts    # Load chat history
â”‚   â”‚       â””â”€â”€ save.ts    # Save chat messages
â”‚   â”œâ”€â”€ _app.tsx           # Application wrapper
â”‚   â””â”€â”€ index.tsx          # Main entry point
â”œâ”€â”€ styles/                # Global styling
â”‚   â””â”€â”€ globals.css        # Tailwind CSS imports
â”œâ”€â”€ types/                 # TypeScript type definitions
â”‚   â””â”€â”€ index.ts           # Shared interfaces and types
â””â”€â”€ Configuration files
```

## Quick Start

### Prerequisites

- **Node.js** 18+ and npm
- **Ollama** installed and running locally
- **Supabase** account (for authentication and data persistence)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/Lordiod/Ollama-web-version.git
   cd ollama-web-version
   ```

2. **Install dependencies**
   ```bash
   npm install
   ```

3. **Set up environment variables**
   ```bash
   cp .env.example .env.local
   ```
   
   Edit `.env.local` and add your Supabase credentials:
   ```env
   NEXT_PUBLIC_SUPABASE_URL=your_supabase_project_url
   NEXT_PUBLIC_SUPABASE_ANON_KEY=your_supabase_anonymous_key
   ```

4. **Set up Ollama**
   ```bash
   # Start Ollama service
   ollama serve
   
   # Pull a model (example with Llama 3.2)
   ollama pull llama3.2:3b
   ```

5. **Start the development server**
   ```bash
   npm run dev
   ```

6. **Open your browser**
   Navigate to `http://localhost:3000`

## Configuration

### Supabase Setup

1. Create a new project at [supabase.com](https://supabase.com)
2. Go to **Settings > API** to find your project URL and anon key
3. Update your `.env.local` file with these credentials
4. Set up the required database tables (see [Database Schema](#database-schema))

### Ollama Configuration

Ensure Ollama is running with your preferred model:

```bash
# List available models
ollama list

# Pull a specific model
ollama pull llama3.2:3b

# Start Ollama (if not running)
ollama serve
```

## Database Schema

The application requires the following Supabase tables:

### Users Table
Handled automatically by Supabase Auth.

### Conversations Table
```sql
create table conversations (
  id uuid default gen_random_uuid() primary key,
  user_id uuid references auth.users(id) on delete cascade,
  title text,
  created_at timestamp with time zone default timezone('utc'::text, now()),
  updated_at timestamp with time zone default timezone('utc'::text, now())
);
```

### Messages Table
```sql
create table messages (
  id uuid default gen_random_uuid() primary key,
  conversation_id uuid references conversations(id) on delete cascade,
  role text check (role in ('user', 'assistant')),
  content text not null,
  created_at timestamp with time zone default timezone('utc'::text, now())
);
```

## Available Scripts

| Command | Description |
|---------|-------------|
| `npm run dev` | Start development server with hot reload |
| `npm run build` | Create production build |
| `npm run start` | Start production server |
| `npm run lint` | Run ESLint code analysis |
| `npm run lint:fix` | Run ESLint and fix issues automatically |
| `npm run type-check` | Verify TypeScript types |

## API Endpoints

### Authentication
- `POST /api/auth/signup` - Register new user
- `POST /api/auth/login` - User login

### Chat Management
- `POST /api/chat` - Send message to AI
- `GET /api/chat/load` - Load chat history
- `POST /api/chat/save` - Save chat message
- `DELETE /api/chat/delete` - Delete chat
- `DELETE /api/chat/clear` - Clear all chats
- `POST /api/chat/generate-title` - Generate chat title

## Deployment

### Vercel (Recommended)

1. **Deploy to Vercel**
   ```bash
   npm install -g vercel
   vercel
   ```

2. **Configure Environment Variables**
   Add your environment variables in the Vercel dashboard:
   - `NEXT_PUBLIC_SUPABASE_URL`
   - `NEXT_PUBLIC_SUPABASE_ANON_KEY`

### Docker Deployment

1. **Build the Docker image**
   ```bash
   docker build -t ollama-web-assistant .
   ```

2. **Run the container**
   ```bash
   docker run -p 3000:3000 \
     -e NEXT_PUBLIC_SUPABASE_URL=your_url \
     -e NEXT_PUBLIC_SUPABASE_ANON_KEY=your_key \
     ollama-web-assistant
   ```

### Manual Deployment

1. **Build the application**
   ```bash
   npm run build
   ```

2. **Start production server**
   ```bash
   npm run start
   ```

## Development

### Code Style

This project uses ESLint for code consistency:

```bash
# Check linting
npm run lint

# Fix linting issues
npm run lint:fix
```

### Type Checking

Run TypeScript type checking:

```bash
npm run type-check
```

## Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.
## Troubleshooting

### Common Issues

**Ollama Connection Failed**
- Verify Ollama is running: `ollama serve`
- Check if model is installed: `ollama list`
- Ensure port 11434 is not blocked

**Supabase Authentication Errors**
- Verify environment variables in `.env.local`
- Check Supabase project status
- Ensure database tables are created

**TypeScript Compilation Errors**
- Run `npm run type-check` for detailed errors
- Verify all dependencies are installed
- Check `tsconfig.json` configuration

**Build or Runtime Errors**
- Clear Next.js cache: `rm -rf .next`
- Reinstall dependencies: `rm -rf node_modules && npm install`
- Check Node.js version compatibility

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- [Ollama](https://ollama.ai/) for providing excellent local AI models
- [Supabase](https://supabase.com/) for the backend infrastructure
- [Next.js](https://nextjs.org/) for the amazing React framework
- [Tailwind CSS](https://tailwindcss.com/) for the utility-first CSS framework
---